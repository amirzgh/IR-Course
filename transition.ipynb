{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "path_direction = \"resources/transition-data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 19/19 [00:00<00:00, 89642.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tqdm\n",
    "\n",
    "data = [x.strip() for x in tqdm.tqdm(codecs.open(F'{path_direction}','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['علی به مدرسه رفت.', 'ما با پیکان جوانان گوجه ای به مشهَدالرضا رفتیم.', 'محمد و دوستش با تیبا از مهاباد به شیراز سفر کردند.', 'فاطمه با ماشین به یزد آمد.', 'اتوبوس به تهران رسید.', 'علی به مقصد اهواز حرکت کرد.', 'خانواده هاشمی با اتوبوس از نیشابور به قم رفتند.', 'محمد رضا با پراید از ماهشهر به تهران رفت.', 'دوستان من برای اردو به شیراز رفتند.', 'سفر با ماشین به خراسان به پایان رسید.', 'نیسان آبی از ارومیه به تبریز رسید.', 'سفر با هواپیما از تهران به شیراز آسان بود.', 'فردا با پژو به مشهد خواهیم رفت.', 'ما با تاکسی به کرج رفتیم.', 'دوستم با هواپیما به یزد سفر کرد.', 'دوستان من با سمند به کرج رفتند.', 'اتوبوس بازیکنان به رشت رسید.', 'مسافران قطار از تهران به مقصد شهرری سوار شدند.', 'دیروز با هواپیما به تهران آمدم.']\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# data_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(datas)]\n",
    "data_normalized = [normalizer.normalize(y) for y in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "علی به مدرسه رفت.\n",
      "ما با پیکان جوانان گوجه‌ای به مشهدالرضا رفتیم.\n",
      "محمد و دوستش با تیبا از مهاباد به شیراز سفر کردند.\n",
      "فاطمه با ماشین به یزد آمد.\n",
      "اتوبوس به تهران رسید.\n",
      "علی به مقصد اهواز حرکت کرد.\n",
      "خانواده هاشمی با اتوبوس از نیشابور به قم رفتند.\n",
      "محمد رضا با پراید از ماهشهر به تهران رفت.\n",
      "دوستان من برای اردو به شیراز رفتند.\n",
      "سفر با ماشین به خراسان به پایان رسید.\n",
      "نیسان آبی از ارومیه به تبریز رسید.\n",
      "سفر با هواپیما از تهران به شیراز آسان بود.\n",
      "فردا با پژو به مشهد خواهیم رفت.\n",
      "ما با تاکسی به کرج رفتیم.\n",
      "دوستم با هواپیما به یزد سفر کرد.\n",
      "دوستان من با سمند به کرج رفتند.\n",
      "اتوبوس بازیکنان به رشت رسید.\n",
      "مسافران قطار از تهران به مقصد شهرری سوار شدند.\n",
      "دیروز با هواپیما به تهران آمدم.\n"
     ]
    }
   ],
   "source": [
    "for x in data_normalized :\n",
    "    print(''.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 19/19 [00:00<00:00, 23902.75it/s]\n"
     ]
    }
   ],
   "source": [
    "data_sentences = [sent_tokenize(''.join(x)) for x in tqdm.tqdm(data_normalized)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 3], [5, 6], [8, 12], [14, 16]], [[1, 2], [4, 5], [7, 11], [13, 18], [20, 26], [28, 29], [31, 39], [41, 45]], [[1, 4], [6, 6], [8, 12], [14, 15], [17, 20], [22, 23], [25, 30], [32, 33], [35, 39], [41, 43], [45, 49]], [[1, 5], [7, 8], [10, 14], [16, 17], [19, 21], [23, 25]], [[1, 6], [8, 9], [11, 15], [17, 20]], [[1, 3], [5, 6], [8, 11], [13, 17], [19, 22], [24, 26]], [[1, 7], [9, 13], [15, 16], [18, 23], [25, 26], [28, 34], [36, 37], [39, 40], [42, 46]], [[1, 4], [6, 8], [10, 11], [13, 17], [19, 20], [22, 27], [29, 30], [32, 36], [38, 40]], [[1, 6], [8, 9], [11, 14], [16, 19], [21, 22], [24, 28], [30, 34]], [[1, 3], [5, 6], [8, 12], [14, 15], [17, 22], [24, 25], [27, 31], [33, 36]], [[1, 5], [7, 9], [11, 12], [14, 19], [21, 22], [24, 28], [30, 33]], [[1, 3], [5, 6], [8, 14], [16, 17], [19, 23], [25, 26], [28, 32], [34, 37], [39, 41]], [[1, 4], [6, 7], [9, 11], [13, 14], [16, 19], [21, 26], [28, 30]], [[1, 2], [4, 5], [7, 11], [13, 14], [16, 18], [20, 24]], [[1, 5], [7, 8], [10, 16], [18, 19], [21, 23], [25, 27], [29, 31]], [[1, 6], [8, 9], [11, 12], [14, 17], [19, 20], [22, 24], [26, 30]], [[1, 6], [8, 15], [17, 18], [20, 22], [24, 27]], [[1, 7], [9, 12], [14, 15], [17, 21], [23, 24], [26, 29], [31, 35], [37, 40], [42, 45]], [[1, 5], [7, 8], [10, 16], [18, 19], [21, 25], [27, 30]]]\n"
     ]
    }
   ],
   "source": [
    "spans = []\n",
    "for i in range(len(data_sentences)):\n",
    "    start = 1\n",
    "    end = 1\n",
    "    current_spans = []\n",
    "    for j in data_sentences[i][0]:\n",
    "        end += 1\n",
    "        if j == \" \" or j == \".\":\n",
    "            current_spans.append([start, end-2])\n",
    "            start = end\n",
    "    spans.append(current_spans)\n",
    "print(spans)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "علی به مدرسه رفت.\n",
      "ما با پیکان جوانان گوجه‌ای به مشهدالرضا رفتیم.\n",
      "محمد و دوستش با تیبا از مهاباد به شیراز سفر کردند.\n",
      "فاطمه با ماشین به یزد آمد.\n",
      "اتوبوس به تهران رسید.\n",
      "علی به مقصد اهواز حرکت کرد.\n",
      "خانواده هاشمی با اتوبوس از نیشابور به قم رفتند.\n",
      "محمد رضا با پراید از ماهشهر به تهران رفت.\n",
      "دوستان من برای اردو به شیراز رفتند.\n",
      "سفر با ماشین به خراسان به پایان رسید.\n",
      "نیسان آبی از ارومیه به تبریز رسید.\n",
      "سفر با هواپیما از تهران به شیراز آسان بود.\n",
      "فردا با پژو به مشهد خواهیم رفت.\n",
      "ما با تاکسی به کرج رفتیم.\n",
      "دوستم با هواپیما به یزد سفر کرد.\n",
      "دوستان من با سمند به کرج رفتند.\n",
      "اتوبوس بازیکنان به رشت رسید.\n",
      "مسافران قطار از تهران به مقصد شهرری سوار شدند.\n",
      "دیروز با هواپیما به تهران آمدم.\n"
     ]
    }
   ],
   "source": [
    "for x in data_sentences:\n",
    "    print('###'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:00<00:00, 96.08it/s]\n"
     ]
    }
   ],
   "source": [
    "data_tokens = [[word_tokenize(sentence) for sentence in sentences] for sentences in tqdm.tqdm(data_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['علی', 'به', 'مدرسه', 'رفت', '.']],\n",
       " [['ما',\n",
       "   'با',\n",
       "   'پیکان',\n",
       "   'جوانان',\n",
       "   'گوجه\\u200cای',\n",
       "   'به',\n",
       "   'مشهدالرضا',\n",
       "   'رفتیم',\n",
       "   '.']],\n",
       " [['محمد',\n",
       "   'و',\n",
       "   'دوستش',\n",
       "   'با',\n",
       "   'تیبا',\n",
       "   'از',\n",
       "   'مهاباد',\n",
       "   'به',\n",
       "   'شیراز',\n",
       "   'سفر',\n",
       "   'کردند',\n",
       "   '.']],\n",
       " [['فاطمه', 'با', 'ماشین', 'به', 'یزد', 'آمد', '.']],\n",
       " [['اتوبوس', 'به', 'تهران', 'رسید', '.']],\n",
       " [['علی', 'به', 'مقصد', 'اهواز', 'حرکت', 'کرد', '.']],\n",
       " [['خانواده',\n",
       "   'هاشمی',\n",
       "   'با',\n",
       "   'اتوبوس',\n",
       "   'از',\n",
       "   'نیشابور',\n",
       "   'به',\n",
       "   'قم',\n",
       "   'رفتند',\n",
       "   '.']],\n",
       " [['محمد', 'رضا', 'با', 'پراید', 'از', 'ماهشهر', 'به', 'تهران', 'رفت', '.']],\n",
       " [['دوستان', 'من', 'برای', 'اردو', 'به', 'شیراز', 'رفتند', '.']],\n",
       " [['سفر', 'با', 'ماشین', 'به', 'خراسان', 'به', 'پایان', 'رسید', '.']],\n",
       " [['نیسان', 'آبی', 'از', 'ارومیه', 'به', 'تبریز', 'رسید', '.']],\n",
       " [['سفر', 'با', 'هواپیما', 'از', 'تهران', 'به', 'شیراز', 'آسان', 'بود', '.']],\n",
       " [['فردا', 'با', 'پژو', 'به', 'مشهد', 'خواهیم_رفت', '.']],\n",
       " [['ما', 'با', 'تاکسی', 'به', 'کرج', 'رفتیم', '.']],\n",
       " [['دوستم', 'با', 'هواپیما', 'به', 'یزد', 'سفر', 'کرد', '.']],\n",
       " [['دوستان', 'من', 'با', 'سمند', 'به', 'کرج', 'رفتند', '.']],\n",
       " [['اتوبوس', 'بازیکنان', 'به', 'رشت', 'رسید', '.']],\n",
       " [['مسافران',\n",
       "   'قطار',\n",
       "   'از',\n",
       "   'تهران',\n",
       "   'به',\n",
       "   'مقصد',\n",
       "   'شهرری',\n",
       "   'سوار',\n",
       "   'شدند',\n",
       "   '.']],\n",
       " [['دیروز', 'با', 'هواپیما', 'به', 'تهران', 'آمدم', '.']]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_database(file_path):\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    return file.read()\n",
    "\n",
    "def read_list_from_file(file_path):\n",
    "    lst = read_database(file_path)\n",
    "    lst = lst.strip('][').split(', ')\n",
    "    lst = lst[0].replace(\"\\\"\", \"\").replace(\",\", \"\").replace(\" \", \"\").split(\"\\n\")\n",
    "    final_list = []\n",
    "    for string in lst:\n",
    "        if (string != \"\"):\n",
    "            final_list.append(string)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read databases of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = read_list_from_file(\"database/cars.txt\")\n",
    "vehicles = read_list_from_file(\"database/vehicle.txt\")\n",
    "vehicles = cars + vehicles\n",
    "vehicles = [normalizer.normalize(y) for y in vehicles]\n",
    "#########\n",
    "countries = read_list_from_file(\"database/countries.txt\")\n",
    "address_keyword = read_list_from_file(\"database/address keyword.txt\")\n",
    "cities = read_list_from_file(\"database/cities.txt\")\n",
    "provincies = read_list_from_file(\"database/provincies.txt\")\n",
    "places = read_list_from_file(\"database/places.txt\")\n",
    "places = countries + address_keyword + provincies + places + cities\n",
    "places = [normalizer.normalize(y) for y in places]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for j in range(len(data_tokens)):\n",
    "    sentence = data_tokens[j]\n",
    "    sentence_full = data[j]\n",
    "    sentence = sentence[0]\n",
    "    res_dict = {}\n",
    "    ###\n",
    "    res_dict[\"from\"] = \"\"\n",
    "    res_dict[\"from_span\"] = \"[-1, -1]\"\n",
    "    res_dict[\"to\"] = \"\"\n",
    "    res_dict[\"to_span\"] = \"[-1, -1]\"\n",
    "    res_dict[\"vehicle\"] = \"\"\n",
    "    res_dict[\"vehicle_span\"] = \"[-1, -1]\"\n",
    "    ###\n",
    "    for i in range(len(sentence)-1):\n",
    "        if sentence[i] == \"از\" or sentence[i] == \"مبدا\":\n",
    "            if sentence[i+1] in places:\n",
    "                res_dict[\"from\"] = sentence[i+1]\n",
    "                res_dict[\"from_span\"] = spans[j][i+1]\n",
    "        if sentence[i] == \"به\" or sentence[i] == \"مقصد\":\n",
    "            if sentence[i+1] in places:\n",
    "                res_dict[\"to\"] = sentence[i+1]\n",
    "                res_dict[\"to_span\"] = spans[j][i+1]\n",
    "        if sentence[i] == \"با\" or sentence[i] == \"خودروی\" or sentence[i] == \"ماشین\":\n",
    "            if sentence[i+1] in vehicles:\n",
    "                res_dict[\"vehicle\"] = sentence[i+1]\n",
    "                res_dict[\"vehicle_span\"] = spans[j][i+1]\n",
    "    result.append(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'مدرسه',\n",
       "  'to_span': [8, 12],\n",
       "  'vehicle': '',\n",
       "  'vehicle_span': '[-1, -1]'},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'مشهدالرضا',\n",
       "  'to_span': [31, 39],\n",
       "  'vehicle': 'پیکان',\n",
       "  'vehicle_span': [7, 11]},\n",
       " {'from': 'مهاباد',\n",
       "  'from_span': [25, 30],\n",
       "  'to': 'شیراز',\n",
       "  'to_span': [35, 39],\n",
       "  'vehicle': 'تیبا',\n",
       "  'vehicle_span': [17, 20]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'یزد',\n",
       "  'to_span': [19, 21],\n",
       "  'vehicle': 'ماشین',\n",
       "  'vehicle_span': [10, 14]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'تهران',\n",
       "  'to_span': [11, 15],\n",
       "  'vehicle': '',\n",
       "  'vehicle_span': '[-1, -1]'},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'اهواز',\n",
       "  'to_span': [13, 17],\n",
       "  'vehicle': '',\n",
       "  'vehicle_span': '[-1, -1]'},\n",
       " {'from': 'نیشابور',\n",
       "  'from_span': [28, 34],\n",
       "  'to': 'قم',\n",
       "  'to_span': [39, 40],\n",
       "  'vehicle': 'اتوبوس',\n",
       "  'vehicle_span': [18, 23]},\n",
       " {'from': 'ماهشهر',\n",
       "  'from_span': [22, 27],\n",
       "  'to': 'تهران',\n",
       "  'to_span': [32, 36],\n",
       "  'vehicle': 'پراید',\n",
       "  'vehicle_span': [13, 17]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'شیراز',\n",
       "  'to_span': [24, 28],\n",
       "  'vehicle': '',\n",
       "  'vehicle_span': '[-1, -1]'},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'خراسان',\n",
       "  'to_span': [17, 22],\n",
       "  'vehicle': 'ماشین',\n",
       "  'vehicle_span': [8, 12]},\n",
       " {'from': 'ارومیه',\n",
       "  'from_span': [14, 19],\n",
       "  'to': 'تبریز',\n",
       "  'to_span': [24, 28],\n",
       "  'vehicle': '',\n",
       "  'vehicle_span': '[-1, -1]'},\n",
       " {'from': 'تهران',\n",
       "  'from_span': [19, 23],\n",
       "  'to': 'شیراز',\n",
       "  'to_span': [28, 32],\n",
       "  'vehicle': 'هواپیما',\n",
       "  'vehicle_span': [8, 14]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'مشهد',\n",
       "  'to_span': [16, 19],\n",
       "  'vehicle': 'پژو',\n",
       "  'vehicle_span': [9, 11]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'کرج',\n",
       "  'to_span': [16, 18],\n",
       "  'vehicle': 'تاکسی',\n",
       "  'vehicle_span': [7, 11]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'یزد',\n",
       "  'to_span': [21, 23],\n",
       "  'vehicle': 'هواپیما',\n",
       "  'vehicle_span': [10, 16]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'کرج',\n",
       "  'to_span': [22, 24],\n",
       "  'vehicle': 'سمند',\n",
       "  'vehicle_span': [14, 17]},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'رشت',\n",
       "  'to_span': [20, 22],\n",
       "  'vehicle': '',\n",
       "  'vehicle_span': '[-1, -1]'},\n",
       " {'from': 'تهران',\n",
       "  'from_span': [17, 21],\n",
       "  'to': 'شهرری',\n",
       "  'to_span': [31, 35],\n",
       "  'vehicle': '',\n",
       "  'vehicle_span': '[-1, -1]'},\n",
       " {'from': '',\n",
       "  'from_span': '[-1, -1]',\n",
       "  'to': 'تهران',\n",
       "  'to_span': [21, 25],\n",
       "  'vehicle': 'هواپیما',\n",
       "  'vehicle_span': [10, 16]}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
