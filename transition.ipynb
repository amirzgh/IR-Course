{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "path_direction = \"resources/transition-data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import tqdm\n",
    "\n",
    "data = [x.strip() for x in tqdm.tqdm(codecs.open(F'{path_direction}','rU','utf-8').readlines())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ما از تهران با پیکان جوانان گوجه ای به مشهَدالرضا رفتیم.']\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# data_normalized = [[normalizer.normalize(y) for y in x] for x in tqdm.tqdm(datas)]\n",
    "data_normalized = [normalizer.normalize(y) for y in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما از تهران با پیکان جوانان گوجه‌ای به مشهدالرضا رفتیم.\n"
     ]
    }
   ],
   "source": [
    "for x in data_normalized :\n",
    "    print(''.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1088.86it/s]\n"
     ]
    }
   ],
   "source": [
    "data_sentences = [sent_tokenize(''.join(x)) for x in tqdm.tqdm(data_normalized)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1, 2], [4, 5], [7, 11], [13, 14], [16, 20], [22, 27], [29, 35], [37, 38], [40, 48], [50, 54]]]\n"
     ]
    }
   ],
   "source": [
    "spans = []\n",
    "for i in range(len(data_sentences)):\n",
    "    start = 1\n",
    "    end = 1\n",
    "    current_spans = []\n",
    "    for j in data_sentences[i][0]:\n",
    "        end += 1\n",
    "        if j == \" \" or j == \".\":\n",
    "            current_spans.append([start, end-2])\n",
    "            start = end\n",
    "    spans.append(current_spans)\n",
    "print(spans)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما از تهران با پیکان جوانان گوجه‌ای به مشهدالرضا رفتیم.\n"
     ]
    }
   ],
   "source": [
    "for x in data_sentences:\n",
    "    print('###'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\n"
     ]
    }
   ],
   "source": [
    "data_tokens = [[word_tokenize(sentence) for sentence in sentences] for sentences in tqdm.tqdm(data_sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['ما',\n",
       "   'از',\n",
       "   'تهران',\n",
       "   'با',\n",
       "   'پیکان',\n",
       "   'جوانان',\n",
       "   'گوجه\\u200cای',\n",
       "   'به',\n",
       "   'مشهدالرضا',\n",
       "   'رفتیم',\n",
       "   '.']]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_database(file_path):\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    return file.read()\n",
    "\n",
    "def read_list_from_file(file_path):\n",
    "    lst = read_database(file_path)\n",
    "    lst = lst.strip('][').split(', ')\n",
    "    lst = lst[0].replace(\"\\\"\", \"\").replace(\",\", \"\").replace(\" \", \"\").split(\"\\n\")\n",
    "    final_list = []\n",
    "    for string in lst:\n",
    "        if (string != \"\"):\n",
    "            final_list.append(string)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read databases of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = read_list_from_file(\"database/cars.txt\")\n",
    "vehicles = read_list_from_file(\"database/vehicle.txt\")\n",
    "vehicles = cars + vehicles\n",
    "vehicles = [normalizer.normalize(y) for y in vehicles]\n",
    "#########\n",
    "countries = read_list_from_file(\"database/countries.txt\")\n",
    "address_keyword = read_list_from_file(\"database/address keyword.txt\")\n",
    "cities = read_list_from_file(\"database/cities.txt\")\n",
    "provincies = read_list_from_file(\"database/provincies.txt\")\n",
    "places = read_list_from_file(\"database/places.txt\")\n",
    "places = countries + address_keyword + provincies + places + cities\n",
    "places = [normalizer.normalize(y) for y in places]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for j in range(len(data_tokens)):\n",
    "    sentence = data_tokens[j]\n",
    "    sentence_full = data[j]\n",
    "    sentence = sentence[0]\n",
    "    res_dict = {}\n",
    "    ###\n",
    "    res_dict[\"from\"] = \"\"\n",
    "    res_dict[\"from_span\"] = \"[-1, -1]\"\n",
    "    res_dict[\"to\"] = \"\"\n",
    "    res_dict[\"to_span\"] = \"[-1, -1]\"\n",
    "    res_dict[\"vehicle\"] = \"\"\n",
    "    res_dict[\"vehicle_span\"] = \"[-1, -1]\"\n",
    "    ###\n",
    "    for i in range(len(sentence)-1):\n",
    "        if sentence[i] == \"از\" or sentence[i] == \"مبدا\":\n",
    "            if sentence[i+1] in places:\n",
    "                res_dict[\"from\"] = sentence[i+1]\n",
    "                res_dict[\"from_span\"] = spans[j][i]\n",
    "        if sentence[i] == \"به\" or sentence[i] == \"مقصد\":\n",
    "            if sentence[i+1] in places:\n",
    "                res_dict[\"to\"] = sentence[i+1]\n",
    "                res_dict[\"to_span\"] = spans[j][i]\n",
    "        if sentence[i] == \"با\" or sentence[i] == \"خودروی\" or sentence[i] == \"ماشین\":\n",
    "            if sentence[i+1] in vehicles:\n",
    "                res_dict[\"vehicle\"] = sentence[i+1]\n",
    "                res_dict[\"vehicle_span\"] = spans[j][i]\n",
    "    result.append(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'from': 'تهران',\n",
       "  'from_span': [4, 5],\n",
       "  'to': 'مشهدالرضا',\n",
       "  'to_span': [37, 38],\n",
       "  'vehicle': 'پیکان',\n",
       "  'vehicle_span': [13, 14]}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}